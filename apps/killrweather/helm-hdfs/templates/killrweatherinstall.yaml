
{{ if eq .Values.components.app "killrweather" }}
apiVersion: v1
kind: Pod
metadata:
  name: killrweatherapp
  labels:
    fdpmanager : visible
spec:
  containers:
  - name: killrweatherapp
    image: {{ .Values.image.app}}:{{.Values.image.version }}
    command:
    - sh
    - -c
    - "/opt/spark/bin/spark-submit
        --master k8s://http://kube-apiserver-0-instance.kubernetes.mesos:9000
        --deploy-mode cluster
        --name killrweather
        --class {{ .Values.app.main }}
        --conf spark.executor.instances=3
        --files http://api.hdfs.marathon.l4lb.thisdcos.directory/v1/endpoints/hdfs-site.xml,http://api.hdfs.marathon.l4lb.thisdcos.directory/v1/endpoints/core-site.xml
        --conf spark.kubernetes.mountDependencies.filesDownloadDir=/etc/hadoop/conf
        --conf 'spark.driver.extraJavaOptions=-Dconfig.resource=cluster.conf'
        --conf 'spark.executor.extraJavaOptions=-Dconfig.resource=cluster.conf'
        --conf spark.kubernetes.container.image={{ .Values.image.app}}:{{.Values.image.version }}
        --conf spark.kubernetes.container.image.pullPolicy={{ .Values.image.pullPolicy}}
        --conf spark.kubernetes.driverEnv.KAFKA_BROKERS={{ .Values.configuration.kafka.brokerlist}}
        --conf spark.kubernetes.driverEnv.CASSANDRA_HOSTS={{ .Values.configuration.cassandra.hosts}}
        --conf spark.kubernetes.driverEnv.SPARK_BATCH_INTERVAL={{ .Values.configuration.streaming.batchInterval}}
        --conf spark.kubernetes.driverEnv.CHECKPOINT_DIRECTORY={{ .Values.configuration.streaming.checkpointDir}}
        --conf spark.kubernetes.driverEnv.GRAFANA_HOST={{ .Values.configuration.grafana.host}}
        --conf spark.kubernetes.driverEnv.GRAFANA_PORT={{ .Values.configuration.grafana.port}}
        --conf spark.kubernetes.driverEnv.INFLUXDB_HOST={{ .Values.configuration.influx.host}}
        --conf spark.kubernetes.driverEnv.INFLUXDB_PORT={{ .Values.configuration.influx.port}}
        --conf 'spark.executorEnv.KAFKA_BROKERS={{ .Values.configuration.kafka.brokerlist}}'
        --conf 'spark.executorEnv.CASSANDRA_HOSTS={{ .Values.configuration.cassandra.hosts}}'
        --conf 'spark.executorEnv.SPARK_BATCH_INTERVAL={{ .Values.configuration.streaming.batchInterval}}'
        --conf 'spark.executorEnv.CHECKPOINT_DIRECTORY={{ .Values.configuration.streaming.checkpointDir}}'
        --conf 'spark.executorEnv.GRAFANA_HOST={{ .Values.configuration.grafana.host}}'
        --conf 'spark.executorEnv.GRAFANA_PORT={{ .Values.configuration.grafana.port}}'
        --conf 'spark.executorEnv.INFLUXDB_HOST={{ .Values.configuration.influx.host}}'
        --conf 'spark.executorEnv.INFLUXDB_PORT={{ .Values.configuration.influx.port}}'
        local:///opt/spark/jars/{{ .Values.app.jar}}:{{.Values.image.version }}.jar"
  restartPolicy: Never

{{else}}

apiVersion: v1
kind: Pod
metadata:
  name: killrweatherappstructured
  labels:
    fdpmanager : visible
spec:
  containers:
  - name: killrweatherappstructured
    image: {{ .Values.image.appstructured}}:{{.Values.image.version }}
    command:
    - sh
    - -c
    - "/opt/spark/bin/spark-submit
        --master k8s://http://kube-apiserver-0-instance.kubernetes.mesos:9000
        --deploy-mode cluster
        --name killrweather
        --class {{ .Values.app.main }}
        --conf spark.executor.instances=3
        --conf 'spark.driver.extraJavaOptions=-Dconfig.resource=cluster.conf'
        --conf 'spark.executor.extraJavaOptions=-Dconfig.resource=cluster.conf'
        --conf spark.kubernetes.container.image={{ .Values.image.app}}:{{.Values.image.version }}
        --conf spark.kubernetes.container.image.pullPolicy={{ .Values.image.pullPolicy}}
        --conf spark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.options.claimName={{.Values.configuration.checkpointing.pvc}}
        --conf spark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.mount.path={{.Values.configuration.streaming.checkpointDir}}
        --conf spark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.mount.readOnly=false
        --conf spark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.options.claimName={{.Values.configuration.checkpointing.pvc}}
        --conf spark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.mount.path={{.Values.configuration.streaming.checkpointDir}}
        --conf spark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.mount.readOnly=false
        --conf spark.kubernetes.driverEnv.KAFKA_BROKERS={{ .Values.configuration.kafka.brokerlist}}
        --conf spark.kubernetes.driverEnv.CASSANDRA_HOSTS={{ .Values.configuration.cassandra.hosts}}
        --conf spark.kubernetes.driverEnv.SPARK_BATCH_INTERVAL={{ .Values.configuration.streaming.batchInterval}}
        --conf spark.kubernetes.driverEnv.CHECKPOINT_DIRECTORY={{ .Values.configuration.streaming.checkpointDir}}
        --conf spark.kubernetes.driverEnv.GRAFANA_HOST={{ .Values.configuration.grafana.host}}
        --conf spark.kubernetes.driverEnv.GRAFANA_PORT={{ .Values.configuration.grafana.port}}
        --conf spark.kubernetes.driverEnv.INFLUXDB_HOST={{ .Values.configuration.influx.host}}
        --conf spark.kubernetes.driverEnv.INFLUXDB_PORT={{ .Values.configuration.influx.port}}
        --conf 'spark.executorEnv.KAFKA_BROKERS={{ .Values.configuration.kafka.brokerlist}}'
        --conf 'spark.executorEnv.CASSANDRA_HOSTS={{ .Values.configuration.cassandra.hosts}}'
        --conf 'spark.executorEnv.SPARK_BATCH_INTERVAL={{ .Values.configuration.streaming.batchInterval}}'
        --conf 'spark.executorEnv.CHECKPOINT_DIRECTORY=//{{ .Values.configuration.streaming.checkpointDir}}'
        --conf 'spark.executorEnv.GRAFANA_HOST={{ .Values.configuration.grafana.host}}'
        --conf 'spark.executorEnv.GRAFANA_PORT={{ .Values.configuration.grafana.port}}'
        --conf 'spark.executorEnv.INFLUXDB_HOST={{ .Values.configuration.influx.host}}'
        --conf 'spark.executorEnv.INFLUXDB_PORT={{ .Values.configuration.influx.port}}'
        local:///opt/spark/jars/{{ .Values.app.jar}}:{{.Values.image.version }}.jar"
  restartPolicy: Never

{{end}}