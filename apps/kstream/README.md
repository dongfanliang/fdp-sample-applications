# Kafka Streams Sample Applications for Web Log Processing

> **Disclaimer:** This sample application is provided as-is, without warranty. It is intended to illustrate techniques for implementing various scenarios using Fast Data Platform, but it has not gone through a robust validation process, nor does it use all the techniques commonly employed for highly-resilient, production applications. Please use it with appropriate caution.

This project consists of two applications that demonstrate the use of Kafka Streams.  Both apps ingest sample web log data from the [Clarknet dataset](http://ita.ee.lbl.gov/html/contrib/ClarkNet-HTTP.html), which is described below.

> These two traces contain two week's worth of all HTTP requests to the ClarkNet WWW server. ClarkNet is a full Internet access provider for the Metro Baltimore-Washington DC area.

The sample application bundle uses the above dataset and has 2 separate main applications, both accessible through http interfaces:

* *WeblogProcessing* - The one based on [Kafka Streams DSL APIs](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html) computes aggregate information from stateful streaming like the total number of bytes transferred for a specific host or the total number of accesses made on a specific host. These can be computed on a windowed aggregation as well.
* *WeblogDriver* - The one based on [Kafka Streams Processor APIs](https://kafka.apache.org/documentation/streams/developer-guide/processor-api.html) implement a custom state store in Kafka Streams to check for membership information. It uses a bloom filter to implement the store on top of the APIs that KS provides. Then it consumes the Clarknet data and gives the user an http interface to check if the application has seen a specific host in its pipeline (membership query).

Together these samples demonstrate the following features of Kafka Streams:

1. Building and configuring a Streams based topology using Kafka Streams DSL as well as the lower level processor based APIs
2. Transformation semantics applied to streams data
3. Stateful transformation using *local* state stores
4. Interactive queries in Kafka streams applied to a distributed application
5. Implementing *custom* state stores
6. Interactive queries over custom state stores in a distributed setting

## Running the Applications Locally

Both the applications can be run locally without deploying in any cluster. The application suite has an embedded Kafka server that can be used to spin up local instances of the applications. Here are the steps that need to be followed:

> Each of the applications `example-dsl` and `example-proc` contains README files that has more details on how to run them locally as single instance application and in distributed mode. Please refer to those documentation for more details.

### Download the ClarkNet Data Files Locally

Download the ClarkNet data files locally, uncompress them and make them available in a folder.

### Update Configuration Files

The bundle contains 2 configuration files that need to be updated for running the applications locally:

1. `kstream/source/core/example-dsl/src/main/resources/application-dsl.conf`
2. `kstream/source/core/example-proc/src/main/resources/application-proc.conf`

Three entries need to be changed here:

1. `kafka.localserver` needs to be set to `true`
2. `kafka.loader.directorytowatch` needs to be set to the folder where the data files have been downloaded
3. `kafka.statestoredir` points to the folder where the state infromation will be generated by Kafka Streams. Ensure this points to a valid writeable location

### Run Locally

The following sequence runs the DSL based application.

```
$ pwd
<..>/kstream/source/core
$ sbt
> dsl
```
One the application starts running, after some time, you can use `curl` to check the http interface based state queries:

`$ curl localhost:7070/weblog/bytes/world.std.com`

The following sequence runs the Processor based application.

```
$ pwd
<..>/kstream/source/core
$ sbt
> proc
```
One the application starts running, after some time, you can use `curl` to check the http interface based state queries:

`$ curl localhost:7070/weblog/access/check/world.std.com`

> For cluster deployment `kafka.localserver` needs to be set to `false`. This is only required to be done if you attempt to deploy the application in the cluster manually instead of using the pre-packaged docker image.

## Deploying and running on DC/OS cluster

The first step in deploying the applications on DC/OS cluster is to prepare docker images of all the applications. This can be done from within sbt.

### Prepare docker images

In the `kstream/source/core/` directory:

```
$ sbt
> projects
[info] In file:/Users/bucktrends/lightbend/fdp-sample-apps/kstream/source/core/
[info] 	   dslRun
[info] 	   fdp-kstream-dsl
[info] 	   fdp-kstream-processor
[info] 	   procRun
[info] 	 * root
[info] 	   server
> project fdp-kstream-dsl
> universal:packageZipTarball
> ...
> docker
```

This will create a docker image named `lightbend/fdp-kstream-dsl:X.Y.Z` (for the current version `X.Y.Z`) with the default settings. The name of the docker repository comes from the `organization` field in `build.sbt` and can be changed there for alternatives. The version of the image comes from `<PROJECT_HOME>/version.sh`. Change there if you wish to deploy a different version.

Once the docker image is created, you can push it to the repository at DockerHub.

### Installing on DC/OS cluster

The installation scripts are present in the `kstream/bin` folder. The script that you need to run is `app-install.sh` which takes a properties file as configuration. The default one is named `app-install.properties`.

```
$ pwd
.../kstream/bin
$ ./app-install.sh --help
  Installs the Kafka Streams sample application. Assumes DC/OS authentication was successful
  using the DC/OS CLI.

  Usage: app-install.sh   [options]

  eg: ./app-install.sh

  Options:
  --config-file        Configuration file used to launch applications
                       Default: ./app-install.properties
  --start-only X       Only start the following apps:
                         dsl         Starts topology based on Kafka Streams DSL
                         processor   Starts topology that implements custom state repository based on Kafka Streams Processor APIs
                       Repeat the option to run more than one.
                       Default: runs all of them
  -n | --no-exec       Do not actually run commands, just print them (for debugging).
  -h | --help          Prints this message.
```

Some of the valid options to install the applications are:

```bash
$ ./app-install.sh --start-only dsl
```

This will install the DSL based module as one of the applications running under Marathon in the DC/OS cluster.

```bash
$ ./app-install.sh --start-only processor
```

This will install the lower level processor based module as one of the applications running under Marathon in the DC/OS cluster.

```bash
$ ./app-install.sh
```

This will install both the modules as applications running under Marathon in the DC/OS cluster.

**If you decide to install and run both the applications together, please ensure your Kafka cluster is beefy enough to handle the load.**

The script `app-install.sh` takes all configuration parameters from a properties file.  The default file is `app-install.properties` which resides in the same directory, but you can specify the file with the `--config-file` argument.  It is recommended that you keep a set of configuration files for personal development, testing, and production.  Simply copy the default file over and modify as needed.

```
## dcos kafka package
kafka-dcos-package=kafka

## dcos service name. Change this if you use a different service name in your
## Kafka installation on DC/OS cluster
kafka-dcos-service-name=kafka

## whether to skip creation of kafka topics - valid values : true | false
skip-create-topics=true

## kafka topic partition : default 1
kafka-topic-partitions=2

## kafka topic replication factor : default 1
kafka-topic-replication-factor=2
```

## Removing the Applications

The `bin` folder contains the script to remove the applications. This script works on the metadata files that the install script generates.

> Make sure that you run the remove script from the same folder you ran the install script

```bash
$ cd bin
$ ./app-remove.sh [--stop-only dsl] [--stop-only processor] [--skip-delete-topics]
```

The above script cleans most of the data that the application generates including (optionally) the topics in Kafka. Still being a Kafka streams application, some of the state that it creates on the local filesystem has to be cleaned manually. Kafka has a utility that helps in this process. Please have a look in this [wiki page](https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Streams+Application+Reset+Tool) for more details.

## Application REST APIs

Both examples run a self-contained http server that can be used to query information from the state stores used in the Kafka Streams pipelines. You can call these API's to determine if the sample applications are running correctly.

### DSL based module

This module demonstrates stateful streaming using local state stores. These state stores can be queried using interactive queries of Kafka streams through the http service developed as part of this application. The ingested data consists of http access logs of a number of URLs - this module supports 2 types of queries on the application state:

1. count of the number of accesses made to a specific host
2. count of the number of bytes transferred to a specific host

Here's how to query using http interface. We are using `curl` for demonstration here:

```bash
$ curl http://10.8.0.9:7070/weblog/access/world.std.com
```

This reports the number of accesses made to the host `world.std.com`.

```bash
$ curl http://10.8.0.9:7070/weblog/bytes/world.std.com
```

This reports the number of bytes transferred to the host `world.std.com`.

> **Note:** In the above `curl` command, the host IP specified has to be one that's accessible from outside the cluster. In case you are not within a VPN, you may need to use a public IP address. Also the ports need to be opened up in the AWS console (if you are running on AWS).

### Processor API based Custom State Store Module

This module demonstrates the use of custom state stores. This module has implemented a state store based on the Bloom Filter data structure. The purpose is to store membership information in a sublinear data structure. And this module offers interactive queries on top of this custom store. The query returns true or false depending on whether the data for the queried host has been stored or not.

Here's how to query using http interface. We are using `curl` for demonstration here:

```bash
$ curl http://10.8.0.9:7070/weblog/access/check/world.std.com
```

This reports `true` if the host `world.std.com` has been seen in the ingested data. Else it returns `false`.

> **Note:** In the above `curl` command, the host IP specified has to be one that's accessible from outside the cluster. In case you are not within a VPN, you may need to use a public IP address. Also the ports need to be opened up in the AWS console (if you are running on AWS).


> **Note:** When deployed on the cluster, Marathon assigns random free ports to both the applications. The actual connection end -points can be found on the DC/OS UI for the corresponding task of the Marathon service. Check out the entry for *Endpoints* under *Configuration* in the *Details* tab of the task.

## Running in Distributed mode

Both the DSL based and Processor based applications can be run in distributed mode. Multiple instances of the application can be run and all state can be queried using any of the host/port combination. Here are some things that need to be taken care of when running any of the applications in the distributed mode:

* The application `id` in the Marathon deployment json must be different for each of the instances. The typical way to ensure this is to use the given `app-install.sh` for installing one instance and then copying the deployment json and changing it for subsequent deployment instances.
* Some of the settings in the `cmd`, `env` and `uris` section of the json need to be different for subsequent deployment instances. Please refer to `kstream-app-dsl-subsequent-instances.json.template` or `kstream-app-proc-subsequent-instances.json.template` for the details of such changes.
* In order to run multiple instances, ensure that all Kafka topics are created with number of partitions > 1 (>= the number of instances run) with replication factors >= 1. Both these factors can be supplied in the `app-install.properties` file during installation.

## Application Restarts

Kafka streams manages state of the application for stateful streaming in local stores. For resiliency these stores are also backed up by Kafka topics, known as *changelog topics*. In case of application restarts, if it happens that the application is restarted ina different node of the cluster, Kafka will recreate the state store based on the compacted information of the changelog topic. This is also useful if the application faces any exception and Mesos does a restart albeit in a different node.

## Avro Serialization and Schema Registry

In order to demonstrate the capabilities of Schema Registry, the DSL module of the application serializes the weblog information using Avro into a Kafka topic named `avro-topic`. However, the schema registry interface has been developed as a pluggable module. In case the Schema Registry service is available, the application uses it for Avro serialization and schema management. In case Schema Registry is not installed, the application falls back to native Avro serialization.

## Interfacing with Confluent Connect

The DSL module of the application generates Avro data corresponding to the ingested records to a topic named `avro-topic`. We can set up Confluent Connect to consume from `avro-topic` and write to HDFS. Confluent repository offers out of the box HDFS Sink connectors that can do this. In this section we will discuss how to set this up in our DC/OS clustered environment.

### Prerequisites for the interface

In order for the interface to work, we need to ensure the following:

1. HDFS is installed in the DC/OS cluster and is up 'n running.
2. We have the custom lightbend universe running as a Marathon service.
3. Lightbend universe is set up as the default Universe (index = 0) in the DC/OS packaging.
4. Modifications for running Confluent Connect with HDFS are available in the installed Lightbend universe (check the `confluent-connect`package in the Universe).
5. Of course Confluent Connect has to run as a mandatory prerequisite.
6. Confluent schema registry service is up 'n running in the DC/OS cluster.

### Installing the Confluent Connect Distributed Worker

This has to be installed from the Lightbend Universe, which should be the default universe once we ensure Step 3 in the last section. Before installing the package, a bunch of internal topics need to be created manually. These topics are used by the Connect worker.

```bash
dcos beta-kafka topic create dcos-connect-configs --replication 3 --partitions 1 --name=kafka
dcos beta-kafka topic create dcos-connect-offsets --replication 3 --partitions 30 --name=kafka
dcos beta-kafka topic create dcos-connect-status --replication 3 --partitions 10 --name=kafka
```

Check that the topics have been created without error.

```bash
dcos beta-kafka topic list --name=kafka
```

The next step is to install the worker from the Universe. Here are some of the steps to install the worker from the DC/OS UI:

1. Select package `confluent-connect v1.0.0-3.2.2` from the Universe
2. Select Advanced Installation since we need to supply the hdfs URL
3. The installation page has a link named *hdfs* in the left pane. Click on the link and fill out *config-url* with the value `http://api.hdfs.marathon.l4lb.thisdcos.directory/v1/endpoints`
4. Complete the installation

Or install from the DC/OS CLI:

Create a DC/OS service override json file `options.json`.  Note that `connect.zookeeper-connect` should be the ZK namespace of the Kafka cluster you wish to use.  `connect.kafka-service` should be the DC/OS service name of the Kafka DC/OS service you wish to use.

```json
{
  "connect": {
    "kafka-service": "kafka",
    "zookeeper-connect": "master.mesos:2181/dcos-service-kafka"
  },
  "hdfs": {
    "config-url": "http://api.hdfs.marathon.l4lb.thisdcos.directory/v1/endpoints"
  }
}
```

Install `confluent-connect`:

```
dcos package install confluent-connect --package-version=1.0.0-3.2.2 --options=options.json
```

The `connect` package should now be available as a Marathon job in the DC/OS UI.

### Installing the HDFS Sink Connector

Once the worker and the internal topics are created, the next step is to create the actual connector. We will be installing the `HdfsSinkConnector` from Confluent which comes as an out of the box connector.

> `HdfsSinkConnector` needs to be configured with a topic from where it will consume Avro data. Please ensure this topic `avro-topic` is pre-installed before we configure the connector. The best way to ensure this is to install the Kafka Streams sample application beforehand which automatically installs all necessary topics.

The only way to install the connector is by using the REST APIs which Connect offers. Here's an example to install our `HdfsSinkConnector`:

```bash
curl -X POST -H "Content-Type: application/json" --data '{"name": "ks-hdfs-sink", "config": {"connector.class":"HdfsSinkConnector", "tasks.max":"1", "hdfs.url":"hdfs://hdfs", "topics":"avro-topic", "flush.size":"1000" }}' http://10.8.0.19:9622/connectors
```
Here the last URL `http://10.8.0.19:9622/connectors` refers to the host / port where the `connect` runs.

For more details on how to configure and manage connectors, have a look at this [Confluent Page](http://docs.confluent.io/current/connect/managing.html).

If the above setup steps went fine, then when you run the application for Kafka Streams DSL module, records will be generated in the `hdfs://hdfs/topics/avro-topic` directory.

Note: To view the contents of avro filese from the command line you can use `avrocat`, which comes with the avro distribution.  There's also a tool called [`fastavro`](https://github.com/tebeka/fastavro) which is more lightweight and easier to get going by installing the pip.

Example)

```bash
# Install fastavro package with pip
pip install fastavro
# Pluck a test file from HDFS
hdfs dfs -get /topics/avro-topic/partition=0/avro-topic+0+0000292000+0000292999.avro avro-topic+0+0000292000+0000292999.avro
# Pretty print the contents
fastavro avro-topic+0+0000292000+0000292999.avro --pretty
```

## Deploying and running on Kubernetes or OpenShift

The first step in running applications on Kubernetes or OpenShift is the step of containerization, which we discussed in the last section. Once the docker images are built we can use Helm Charts to deploy the applications.

All helm charts are created in the `bin/helm` folder of the respective application. Here's a sample of how to deploy all components of `kstream` application into Kubernetes or OpenShift using the helm chart:

```
$ pwd
.../kstream
$ cd bin
$ helm install --name kstream ./helm
...
$ kubectl logs <pod name where the application runs>
```

Same technique can be used to deploy all the sample applications in Kubernetes or OpenShift.

### Using the traefik Ingress Controller

`kstream` application publishes a REST end point and allows users to query on various application state information. In Kubernetes or OpenShift this can be done in a way so that the user gets one fixed end point irrespective of the ways the underlying PODs are distributed across the cluster. Even in case of PODs getting restarted on a different node, it would be great to still have the fixed end point. We use the traefik ingress controller for this.

> Ensure that the traefik ingress controller is installed in the cluster. Check that the dashboard is accessible through some end points like http://<public agent IP>:9901/dashboard/

The helm chart for `kstream` application is configured to work with traefik ingress controller. However the query from the REST endpoints need to be changed as follows:

```
# for kstream DSL
$ curl http://<public agent IP>:9900/kstreamdsl/weblog/access/check/world.std.com

# for kstream procedure
$ curl http://<public agent IP>:9900/kstreamproc/weblog/access/check/world.std.com
```

Note the following:

* The IP address specified is the one from which you can access the dashboard. This is configured when you install traefik in the cluster
* Every http service will be accessible through this IP address and the port combination
* The prefixes `/kstreamdsl` and `/kstreamproc` are what distinguishes the services and send them to the appropriate backend controllers. All these prefixes are defined in the helm chart

