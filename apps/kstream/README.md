# Kafka Streams Sample Applications for Web Log Processing

> **DISCLAIMER:** This sample application is provided as-is, without warranty. It is intended to illustrate techniques for implementing various scenarios using Fast Data Platform, but it has not gone through a robust validation process, nor does it use all the techniques commonly employed for highly-resilient, production applications. Please use it with appropriate caution.

This project consists of two applications that demonstrate the use of Kafka Streams.  Both apps ingest sample web log data from the [Clarknet dataset](http://ita.ee.lbl.gov/html/contrib/ClarkNet-HTTP.html), which is described below.

> These two traces contain two week's worth of all HTTP requests to the ClarkNet WWW server. ClarkNet is a full Internet access provider for the Metro Baltimore-Washington DC area.

The sample application bundle uses the above dataset and has two separate main applications, both accessible through HTTP interfaces:

* *WeblogProcessing* - The one based on [Kafka Streams DSL APIs](https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html) computes aggregate information from stateful streaming like the total number of bytes transferred for a specific host or the total number of accesses made on a specific host. These can be computed on a windowed aggregation as well.
* *WeblogDriver* - The one based on [Kafka Streams Processor APIs](https://kafka.apache.org/documentation/streams/developer-guide/processor-api.html) implement a custom state store in Kafka Streams to check for membership information. It uses a bloom filter to implement the store on top of the APIs that KS provides. Then it consumes the Clarknet data and gives the user an http interface to check if the application has seen a specific host in its pipeline (membership query).

Together these samples demonstrate the following features of Kafka Streams:

1. Building and configuring a Streams based topology using Kafka Streams DSL as well as the lower level processor based APIs
2. Transformation semantics applied to streams data
3. Stateful transformation using *local* state stores
4. Interactive queries in Kafka streams applied to a distributed application
5. Implementing *custom* state stores
6. Interactive queries over custom state stores in a distributed setting

## Running the Applications Locally

Both the applications can be run locally without deploying in any cluster. The application suite has an embedded Kafka server that can be used to spin up local instances of the applications. Here are the steps that need to be followed:

> **Note:** Each of the applications under `source/core`, `example-dsl` and `example-proc`, contains a README file that has more details on how to run the app locally as a single instance application and in distributed mode. Here we'll provide more general details, including execution in OpenShift and Kubernetes.

### Download the ClarkNet Data Files Locally

Download the [Clarknet dataset](http://ita.ee.lbl.gov/html/contrib/ClarkNet-HTTP.html), uncompress the archives, and make them available in a folder.

### Update Configuration Files

The bundle contains 2 configuration files that need to be updated for running the applications locally:

1. `kstream/source/core/example-dsl/src/main/resources/application-dsl.conf`
2. `kstream/source/core/example-proc/src/main/resources/application-proc.conf`

Three entries need to be changed here:

1. `kafka.localserver` needs to be set to `true`.
2. `kafka.loader.directorytowatch` needs to be set to the folder where the data files have been downloaded.
3. `kafka.statestoredir` points to the folder where the state infromation will be generated by Kafka Streams. Ensure this points to a writeable location.

### Run Locally

The following sequence runs the DSL based application.

```bash
$ pwd
.../kstream/source/core
$ sbt
> dsl
```
One the application starts running, after some time, you can use `curl` (in a different terminal) to check the http interface based state queries:

```bash
curl localhost:7070/weblog/bytes/world.std.com
```

Similarly, at the `sbt` prompt, the `proc` task runs the Processor based application:

```bash
...
> proc
```

Once the application starts running, after some time, you can use `curl` to check the http interface based state queries:

```bash
curl localhost:7070/weblog/access/check/world.std.com
```

> **Note:** For cluster deployments, `kafka.localserver` needs to be set to `false`. This is only required to be done if you attempt to deploy the application in the cluster manually instead of using the pre-packaged Docker image.

## Deploying and running on OpenShift or Kubernetes

You can use Lightbend's prebuilt Docker images or build your own, if you've made modifications.

Building the app can be done using the convenient `build.sh` or `sbt`.

For `build.sh`, use one of the following commands:

```bash
build.sh
build.sh --push-docker-images
```

Both effectively run `sbt clean compile docker`, while the second variant also pushes the images to your Docker Hub account. _Only use this option_ if you first change `organization in ThisBuild := CommonSettings.organization` to `organization in ThisBuild := "myorg"` in `source/core/build.sbt`!

To use `sbt` directly:

```bash
$ cd source/core
$ sbt
> docker
```

This will create Docker images named `lightbend/fdp-kstream-dsl:X.Y.Z` and `lightbend/fdp-kstream-processor:X.Y.Z`, for the current version `X.Y.Z`. The name of the docker repository comes from the `organization` field in `build.sbt` and must be changed if you want to upload your image to a public repository. (It's also confusing if your version has code changes compared to the "official" Lightbend builds.).

You can use the `sbt` target `dockerPush` to push the images to Docker Hub, but only after changing the `organization` as just described. You can publish to your local (machine) repo with the `docker:publishLocal` target.

For IDE users, just import a project and use IDE commands.

## Deploying and Running on OpenShift or Kubernetes

Deploy the Docker Images with the Helm Charts in the `helm` directory. If you aren't using the Lightbend Docker images, modify the chart files (e.g., `helm/values.yaml`) as required. The following example deploys all components of the `kstream` application into OpenShift or Kubernetes:

```bash
$ pwd
.../kstream
$ helm install --name kstream ./helm
...
$ kubectl logs <pod name where the application runs>
```

### Exposing REST APIs on OpenShift or Kubernetes

The `kstream` application publishes a REST end point and allows users to query various application state information. To expose the service on "vanilla" Kubernetes use Ingress. On OpenShift, use Route.

Exposing the service gives the user one fixed end point, irrespective of the ways the underlying PODs are distributed across the cluster. Even when PODs are restarted on a different node, it's highly desirable to still have the single fixed end point.

Both examples run a self-contained HTTP server that can be used to query information from the state stores used in the Kafka Streams pipelines. You can call these API's to determine if the sample applications are running correctly.

#### DSL-based Module

This module demonstrates stateful streaming using local state stores. These state stores can be queried using interactive queries of Kafka streams through the HTTP service developed as part of this application. The ingested data consists of HTTP access logs of a number of URLs - this module supports 2 types of queries on the application state:

1. count of the number of accesses made to a specific host
2. count of the number of bytes transferred to a specific host

Here's how to query using HTTP interface. We are using `curl` for demonstration here:

```bash
$ curl http://<host>:7070/weblog/access/world.std.com
```

This reports the number of accesses made to the host `world.std.com`.

```bash
$ curl http://<host>:7070/weblog/bytes/world.std.com
```

This reports the number of bytes transferred to the host `world.std.com`.

> **Note:** In the above `curl` command, the host IP specified has to be one that's accessible from outside the cluster. In case you are not within a VPN, you may need to use a public IP address. Also the ports need to be opened up in the AWS console (if you are running on AWS).

### Processor API based Custom State Store Module

This module demonstrates the use of custom state stores. This module has implemented a state store based on the Bloom Filter data structure. The purpose is to store membership information in a sublinear data structure. And this module offers interactive queries on top of this custom store. The query returns true or false depending on whether the data for the queried host has been stored or not.

Here's how to query using HTTP interface:

```bash
$ curl http://<host>:7070/weblog/access/check/world.std.com
```

This reports `true` if the host `world.std.com` has been seen in the ingested data. Else it returns `false`.

## Running in Distributed Mode

Both the DSL based and Processor based applications can be run in distributed mode. Multiple instances of the application can be run and all state can be queried using any of the host/port combination. Here are some things that need to be taken care of when running any of the applications in the distributed mode:

* Change the `replicaCount` in the Helm Chart `helm/values.yaml` file to a number greater than 1.
* In order to run multiple instances, ensure that all Kafka topics are created with a number of partitions greater than or equal to the replica count 1.

## Application Restarts

Kafka streams manages state of the application for stateful streaming in local stores. For resiliency these stores are also backed up by Kafka topics, known as *changelog topics*. In case of application restarts, if it happens that the application is restarted ina different node of the cluster, Kafka will recreate the state store based on the compacted information of the changelog topic. This is also useful if the application faces any exception and Mesos does a restart albeit in a different node.

## Avro Serialization and Schema Registry

In order to demonstrate the capabilities of Schema Registry, the DSL module of the application serializes the weblog information using Avro into a Kafka topic named `avro-topic`. However, the schema registry interface has been developed as a pluggable module. In case the Schema Registry service is available, the application uses it for Avro serialization and schema management. In case Schema Registry is not installed, the application falls back to native Avro serialization.

